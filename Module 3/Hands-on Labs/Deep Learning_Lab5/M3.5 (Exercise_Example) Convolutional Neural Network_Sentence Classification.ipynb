{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAIST Summer Session 2018\n",
    "\n",
    "## Sentense Classification using Convolutional Neural Network (08.20.2018)\n",
    "\n",
    "- This dataset is obtained from http://cogcomp.org/Data/QA/QC/. This consists of question sentences classified into 6 types (whether the question is about person, location, numeric information, etc.).\n",
    "- This code is adapted from https://github.com/DSKSD/DeepNLP-models-Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim      # need to install      # pip install gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch\n",
    "        \n",
    "def pad_to_batch(batch):\n",
    "    x,y, y_sub = zip(*batch)\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    x_p = []\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1) < max_x:\n",
    "            x_p.append(torch.cat([x[i], torch.LongTensor([word2index['<PAD>']] * (max_x - x[i].size(1))).view(1, -1)], 1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "    return torch.cat(x_p), torch.cat(y).view(-1), torch.cat(y_sub).view(-1)\n",
    "\n",
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs[1::])\n",
    "\n",
    "def getKeysByValue(dictOfElements, valueToFind):\n",
    "    listOfKeys = list()\n",
    "    listOfItems = dictOfElements.items()\n",
    "    for item  in listOfItems:\n",
    "        if item[1] == valueToFind:\n",
    "            listOfKeys.append(item[0])\n",
    "    return  listOfKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('sentence classification/train_5500.label.txt', 'r', encoding='latin-1').readlines()\n",
    "data = [[d.split(':')[1][:-1], d.split(':')[0]] for d in data]\n",
    "X, y = list(zip(*data))\n",
    "y_sub = []\n",
    "X = list(X)\n",
    "for i in range(len(X)):\n",
    "    y_sub.append(X[i].split(' ')[0])\n",
    "\n",
    "\n",
    "for i, x in enumerate(X):\n",
    "    X[i] = re.sub('\\d', '#', x).split()\n",
    "    \n",
    "    \n",
    "vocab = list(set(flatten(X)))\n",
    "\n",
    "word2index={'<PAD>': 0, '<UNK>': 1}\n",
    "\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "        \n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "target2index = {}\n",
    "target_sub2index = {}\n",
    "\n",
    "for cl in set(y):\n",
    "    if target2index.get(cl) is None:\n",
    "        target2index[cl] = len(target2index)\n",
    "        \n",
    "for cl in set(y_sub):\n",
    "    if target_sub2index.get(cl) is None:\n",
    "        target_sub2index[cl] = len(target_sub2index)\n",
    "\n",
    "index2target = {v:k for k, v in target2index.items()}\n",
    "index2target_sub = {v:k for k, v in target_sub2index.items()}\n",
    "\n",
    "\n",
    "X_p, y_p, y_sub_p = [], [], []\n",
    "\n",
    "for pair in zip(X, y, y_sub):\n",
    "    X_p.append(prepare_sequence(pair[0], word2index).view(1, -1))\n",
    "    y_p.append(torch.LongTensor([target2index[pair[1]]]).view(1, -1))\n",
    "    y_sub_p.append(torch.LongTensor([target_sub2index[pair[2]]]).view(1, -1))\n",
    "\n",
    "\n",
    "    \n",
    "data_p = list(zip(X_p, y_p, y_sub_p))\n",
    "data_p_5 = []\n",
    "\n",
    "for i in range(len(data_p)):\n",
    "    if len(data_p[i][0].data.tolist()[0])>= 5 :\n",
    "        data_p_5.append(data_p[i])\n",
    "\n",
    "random.shuffle(data_p_5)\n",
    "train_data = data_p_5[: int(len(data_p_5) * 0.9)]\n",
    "test_data = data_p_5[int(len(data_p_5) * 0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's look inside the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence:  What two countries contain Sierra Nevada mountains ? \n",
      "True category:  ['LOC']\n",
      "True sub_category:  ['country']\n",
      "\n",
      "Source sentence:  The name of the actor who played the detective in the film Kindergarden Cop is what ? \n",
      "True category:  ['HUM']\n",
      "True sub_category:  ['ind']\n",
      "\n",
      "Source sentence:  What color flies closest to the staff on Belgium 's flag ? \n",
      "True category:  ['ENTY']\n",
      "True sub_category:  ['color']\n",
      "\n",
      "Source sentence:  The film `` Jaws '' was made in what year ? \n",
      "True category:  ['NUM']\n",
      "True sub_category:  ['date']\n",
      "\n",
      "Source sentence:  What gate opened on East and West Berlin ? \n",
      "True category:  ['LOC']\n",
      "True sub_category:  ['other']\n",
      "\n",
      "Source sentence:  What two states is Washington D.C. between ? \n",
      "True category:  ['LOC']\n",
      "True sub_category:  ['state']\n",
      "\n",
      "Source sentence:  What southwestern state is dubbed The Silver State ? \n",
      "True category:  ['LOC']\n",
      "True sub_category:  ['state']\n",
      "\n",
      "Source sentence:  At what age did Rossini stop writing opera ? \n",
      "True category:  ['NUM']\n",
      "True sub_category:  ['period']\n",
      "\n",
      "Source sentence:  What are the most albums sold by one artist or band ? \n",
      "True category:  ['ENTY']\n",
      "True sub_category:  ['cremat']\n",
      "\n",
      "Source sentence:  What arch can you see from the Place de la Concorde ? \n",
      "True category:  ['LOC']\n",
      "True sub_category:  ['other']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nDefinition of Question Classes\\n    ABBR = ABBREVIATION: abbreviation\\n    ENTY = ENTITY: entities\\n    DESC = DESCRIPTION: description and abstract concepts\\n    HUM = HUMAN: human beings\\n    LOC = LOCATION: locations\\n    NUM = NUMERIC: numeric values\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    ran = random.randint(0, len(train_data)-1)\n",
    "    print('Source sentence: ', end=' ')\n",
    "    for j in range(len(train_data[ran][0].data.tolist()[0])):\n",
    "        word_find = getKeysByValue(word2index, train_data[ran][0].data.tolist()[0][j])\n",
    "        print(word_find[0], end=' ')\n",
    "    print('\\nTrue category: ', getKeysByValue(target2index, train_data[ran][1].data.tolist()[0][0]))\n",
    "    print('True sub_category: ', getKeysByValue(target_sub2index, train_data[ran][2].data.tolist()[0][0]))\n",
    "    print(\"\")\n",
    "    \n",
    "'''\n",
    "Definition of Question Classes\n",
    "    ABBR = ABBREVIATION: abbreviation\n",
    "    ENTY = ENTITY: entities\n",
    "    DESC = DESCRIPTION: description and abstract concepts\n",
    "    HUM = HUMAN: human beings\n",
    "    LOC = LOCATION: locations\n",
    "    NUM = NUMERIC: numeric values\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, kernel_dim=100, kernel_sizes=(3, 4, 5), dropout=0.5):\n",
    "        super(CNN,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_dim, (K, embedding_dim)) for K in kernel_sizes])\n",
    "\n",
    "        # kernal_size = (K,D) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * kernel_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def init_weights(self, pretrained_word_vectors, is_static=False):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        if is_static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, inputs, is_training=False):\n",
    "        inputs = self.embedding(inputs).unsqueeze(1) # (B,1,T,D)\n",
    "        inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "        concated = torch.cat(inputs, 1)\n",
    "\n",
    "        if is_training:\n",
    "            concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
    "        out = self.fc(concated) \n",
    "        return F.log_softmax(out,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a Loss Function and Optimizer\n",
    "\n",
    "- Random initialization for word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_pretrained = []\n",
    "\n",
    "for key in word2index.keys():\n",
    "    without_pretrained.append(np.random.randn(300))\n",
    "\n",
    "without_pretrained_vectors = np.vstack(without_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "BATCH_SIZE = 50\n",
    "KERNEL_SIZES = [3,4,5]\n",
    "KERNEL_DIM = 100\n",
    "Learning_Rate = 0.001\n",
    "\n",
    "# Instantiate CNN model\n",
    "model = CNN(len(word2index), 300, len(target2index), KERNEL_DIM, KERNEL_SIZES).to(device)\n",
    "model.init_weights(without_pretrained_vectors) # randomly initialize embedding matrix\n",
    "\n",
    "\n",
    "# Set loss and optimizer function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=Learning_Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5] mean_loss : 2.22\n",
      "[1/5] mean_loss : 0.41\n",
      "[2/5] mean_loss : 0.26\n",
      "[3/5] mean_loss : 0.14\n",
      "[4/5] mean_loss : 0.08\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        inputs,targets, targets_sub = pad_to_batch(batch)\n",
    "        \n",
    "        preds = model(inputs, True)        \n",
    "        loss = criterion(preds, targets)\n",
    "        losses.append(loss.data)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, n_epochs, np.mean(losses)))\n",
    "            losses = []          \n",
    "            \n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.01886792452831\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "\n",
    "for test in test_data:\n",
    "    pred = model(test[0]).max(1)[1]\n",
    "    pred = pred.data.tolist()[0]\n",
    "    target = test[1].data.tolist()[0][0]\n",
    "    if pred == target:\n",
    "        accuracy += 1\n",
    "\n",
    "print(accuracy/len(test_data) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's see how the model works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence:  What movie has made the most money ? \n",
      "True category:  ['ENTY']\n",
      "Predicted category:  ['ENTY']\n",
      "\n",
      "Source sentence:  What are faults in the earth 's crust ? \n",
      "True category:  ['DESC']\n",
      "Predicted category:  ['HUM']\n",
      "\n",
      "Source sentence:  Aspartame is also called what ? \n",
      "True category:  ['ENTY']\n",
      "Predicted category:  ['ENTY']\n",
      "\n",
      "Source sentence:  What was the name of the ball game played by the mayans ? \n",
      "True category:  ['ENTY']\n",
      "Predicted category:  ['ENTY']\n",
      "\n",
      "Source sentence:  What costume designer decided that Michael Jackson should only wear one glove ? \n",
      "True category:  ['HUM']\n",
      "Predicted category:  ['ENTY']\n",
      "\n",
      "Source sentence:  What do penguins eat ? \n",
      "True category:  ['ENTY']\n",
      "Predicted category:  ['ENTY']\n",
      "\n",
      "Source sentence:  What is a fear of fish ? \n",
      "True category:  ['ENTY']\n",
      "Predicted category:  ['ENTY']\n",
      "\n",
      "Source sentence:  What 's the mystery of the Bermuda Triangle ? \n",
      "True category:  ['DESC']\n",
      "Predicted category:  ['ENTY']\n",
      "\n",
      "Source sentence:  How can I get in touch with Michael Moore of `` Roger & Me '' ? \n",
      "True category:  ['DESC']\n",
      "Predicted category:  ['DESC']\n",
      "\n",
      "Source sentence:  What is the VDRL test of blood ? \n",
      "True category:  ['DESC']\n",
      "Predicted category:  ['ENTY']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nDefinition of Question Classes\\n    ABBR = ABBREVIATION: abbreviation\\n    ENTY = ENTITY: entities\\n    DESC = DESCRIPTION: description and abstract concepts\\n    HUM = HUMAN: human beings\\n    LOC = LOCATION: locations\\n    NUM = NUMERIC: numeric values\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    ran = random.randint(0, len(test_data)-1)\n",
    "    print('Source sentence: ', end=' ')\n",
    "    for j in range(len(test_data[ran][0].data.tolist()[0])):\n",
    "        word_find = getKeysByValue(word2index, test_data[ran][0].data.tolist()[0][j])\n",
    "        print(word_find[0], end=' ')\n",
    "    print('\\nTrue category: ', getKeysByValue(target2index, test_data[ran][1].data.tolist()[0][0]))\n",
    "    pred = model(test_data[ran][0]).max(1)[1]\n",
    "    pred = pred.data.tolist()[0]\n",
    "    print('Predicted category: ', getKeysByValue(target2index, pred))\n",
    "    print(\"\")\n",
    "    \n",
    "'''\n",
    "Definition of Question Classes\n",
    "    ABBR = ABBREVIATION: abbreviation\n",
    "    ENTY = ENTITY: entities\n",
    "    DESC = DESCRIPTION: description and abstract concepts\n",
    "    HUM = HUMAN: human beings\n",
    "    LOC = LOCATION: locations\n",
    "    NUM = NUMERIC: numeric values\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Leveraging Pre-trained Model\n",
    "- We use pre-trained word vectors (Simple English), trained on Wikipedia using fastText.\n",
    "- You should download the bin file (over 1GB) from https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = FastText.load_fasttext_format('sentence classification/wiki.simple.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('professorial', 0.8434826135635376), ('professorship', 0.8112963438034058), ('professors', 0.7894449234008789), ('profesor', 0.7348014116287231), ('profess', 0.6934148669242859), ('emeritus', 0.675474226474762), ('professed', 0.671900749206543), ('professeur', 0.6563084721565247), ('faculty', 0.5926684141159058), ('university', 0.5912307500839233)]\n",
      "\n",
      "[('studenţesc', 0.8113662600517273), ('studenten', 0.7857683897018433), ('students', 0.7746243476867676), ('studer', 0.6796103119850159), ('studentenverbindung', 0.625778079032898), ('teacher', 0.5754092931747437), ('enroll', 0.562594473361969), ('undergraduates', 0.5616234540939331), ('teachers', 0.5576453804969788), ('school', 0.5531904101371765)]\n",
      "\n",
      "Similarity between student and professor is 0.4570124\n",
      "Similarity between student and teacher is 0.5754093\n",
      "Similarity between student and pirate is 0.07853423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_model.most_similar('professor'))\n",
    "print('')\n",
    "print(pretrained_model.most_similar('student'))\n",
    "\n",
    "print('')\n",
    "print('Similarity between student and professor is', pretrained_model.similarity('professor', 'student'))\n",
    "print('Similarity between student and teacher is', pretrained_model.similarity('teacher', 'student'))\n",
    "print('Similarity between student and pirate is', pretrained_model.similarity('pirate', 'student'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = []\n",
    "\n",
    "for key in word2index.keys():\n",
    "    try:\n",
    "        pretrained.append(pretrained_model[word2index[key]])\n",
    "    except:\n",
    "        pretrained.append(np.random.randn(300))\n",
    "        \n",
    "pretrained_vectors = np.vstack(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(len(word2index), 300, len(target2index), KERNEL_DIM, KERNEL_SIZES).to(device)\n",
    "model.init_weights(pretrained_vectors) # initialize embedding matrix using pretrained vectors\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=Learning_Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5] mean_loss : 2.03\n",
      "[1/5] mean_loss : 0.52\n",
      "[2/5] mean_loss : 0.36\n",
      "[3/5] mean_loss : 0.11\n",
      "[4/5] mean_loss : 0.10\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        inputs,targets, targets_sub = pad_to_batch(batch)\n",
    "                \n",
    "        preds = model(inputs, True)        \n",
    "        loss = criterion(preds, targets)\n",
    "        losses.append(loss.data)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, n_epochs, np.mean(losses)))\n",
    "            losses = []          \n",
    "            \n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.01886792452831\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "\n",
    "for test in test_data:\n",
    "    pred = model(test[0]).max(1)[1]\n",
    "    pred = pred.data.tolist()[0]\n",
    "    target = test[1].data.tolist()[0][0]\n",
    "    if pred == target:\n",
    "        accuracy += 1\n",
    "\n",
    "print(accuracy/len(test_data) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. (Exercise) Classifying  Sub-Caterory\n",
    "- Replace target_2index to target_sub2index\n",
    "- Replace targets to targets_sub\n",
    "- How is the classification performance for more detailed 47 sub-categories, compared to six categories?\n",
    "- Let's try to improve the classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CNN model\n",
    "model = CNN(len(word2index), 300, len(target_sub2index), KERNEL_DIM, KERNEL_SIZES).to(device)\n",
    "model.init_weights(pretrained_vectors) # randomly initialize embedding matrix\n",
    "\n",
    "\n",
    "# Set loss and optimizer function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=Learning_Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5] mean_loss : 4.17\n",
      "[1/5] mean_loss : 1.58\n",
      "[2/5] mean_loss : 0.90\n",
      "[3/5] mean_loss : 0.35\n",
      "[4/5] mean_loss : 0.29\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Define the training loop\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        inputs,targets, targets_sub = pad_to_batch(batch)\n",
    "                \n",
    "        preds = model(inputs, True)        \n",
    "        loss = criterion(preds, targets_sub)\n",
    "        losses.append(loss.data)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, n_epochs, np.mean(losses)))\n",
    "            losses = []          \n",
    "            \n",
    "print(\"Learning finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below is for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.84905660377359\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "\n",
    "for test in test_data:\n",
    "    pred = model(test[0]).max(1)[1]\n",
    "    pred = pred.data.tolist()[0]\n",
    "    target_sub = test[2].data.tolist()[0][0]\n",
    "    if pred == target_sub:\n",
    "        accuracy += 1\n",
    "\n",
    "print(accuracy/len(test_data) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence:  How many members are in the California congressional delegation ? \n",
      "True sub_category:  ['count']\n",
      "Predicted sub_category:  ['count']\n",
      "\n",
      "Source sentence:  What year did Rossetti paint `` Beata Beatrix '' ? \n",
      "True sub_category:  ['date']\n",
      "Predicted sub_category:  ['date']\n",
      "\n",
      "Source sentence:  What kind of organization is ` Last Chance for Animals ' ? \n",
      "True sub_category:  ['gr']\n",
      "Predicted sub_category:  ['desc']\n",
      "\n",
      "Source sentence:  Who taught Matt Murdock to use his extraordinary abilities in Marvel comics ? \n",
      "True sub_category:  ['ind']\n",
      "Predicted sub_category:  ['ind']\n",
      "\n",
      "Source sentence:  When did Lucelly Garcia , a former ambassador of Columbia to Honduras , die ? \n",
      "True sub_category:  ['date']\n",
      "Predicted sub_category:  ['date']\n",
      "\n",
      "Source sentence:  What two major world religions began in India ? \n",
      "True sub_category:  ['religion']\n",
      "Predicted sub_category:  ['other']\n",
      "\n",
      "Source sentence:  What food did Marco Polo introduce into Italy from the court of Kubla Khan ? \n",
      "True sub_category:  ['food']\n",
      "Predicted sub_category:  ['other']\n",
      "\n",
      "Source sentence:  What was the sequel to The Moon 's Balloon ? \n",
      "True sub_category:  ['cremat']\n",
      "Predicted sub_category:  ['ind']\n",
      "\n",
      "Source sentence:  How can I gain access to a spy satelite ? \n",
      "True sub_category:  ['manner']\n",
      "Predicted sub_category:  ['manner']\n",
      "\n",
      "Source sentence:  Why do USA fax machines not work in UK , NNP ? \n",
      "True sub_category:  ['reason']\n",
      "Predicted sub_category:  ['reason']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    ran = random.randint(0, len(test_data)-1)\n",
    "    print('Source sentence: ', end=' ')\n",
    "    for j in range(len(test_data[ran][0].data.tolist()[0])):\n",
    "        word_find = getKeysByValue(word2index, test_data[ran][0].data.tolist()[0][j])\n",
    "        print(word_find[0], end=' ')\n",
    "    print('\\nTrue sub_category: ', getKeysByValue(target_sub2index, test_data[ran][2].data.tolist()[0][0]))\n",
    "    pred = model(test_data[ran][0]).max(1)[1]\n",
    "    pred = pred.data.tolist()[0]\n",
    "    print('Predicted sub_category: ', getKeysByValue(target_sub2index, pred))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
